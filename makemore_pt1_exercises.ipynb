{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "xs_b, ys_b = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs_b.append(stoi[ch1])\n",
    "        ys_b.append(stoi[ch2])\n",
    "        \n",
    "xs_b = torch.tensor(xs_b)\n",
    "ys_b = torch.tensor(ys_b)\n",
    "num = xs_b.shape[0]\n",
    "print('number of examples:', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "# trigram\n",
    "xs_t, ys_t = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        xs_t.append([stoi[ch1], stoi[ch2]])\n",
    "        ys_t.append(stoi[ch3])\n",
    "        \n",
    "xs_t = torch.tensor(xs_t)\n",
    "ys_t = torch.tensor(ys_t)\n",
    "num = xs_t.shape[0]\n",
    "print('number of examples:', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.409698009490967\n",
      "2.4073660373687744\n",
      "2.405068874359131\n",
      "2.4028050899505615\n",
      "2.400573492050171\n",
      "2.39837384223938\n",
      "2.3962056636810303\n",
      "2.3940680027008057\n",
      "2.391960382461548\n",
      "2.3898818492889404\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    bigram_indices = xs_t[:, 0] * 27 + xs_t[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "    xenc = F.one_hot(bigram_indices, 27*27).float() # one-hot encoding\n",
    "    \n",
    "    logits = (xenc @ W) # log-counts\n",
    "    counts = logits.exp() # equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "    loss = -probs[torch.arange(len(ys_t)), ys_t].log().mean() + 0.01*(W**2).mean() # negative log-likelihood\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set the gradients to zero\n",
    "    loss.backward() # backpropagate\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad # update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0098, 0.0209, 0.0125,  ..., 0.0064, 0.0027, 0.0299],\n",
      "        [0.1306, 0.1681, 0.0358,  ..., 0.0118, 0.0309, 0.0058],\n",
      "        [0.0609, 0.1708, 0.0103,  ..., 0.0038, 0.0987, 0.0122],\n",
      "        ...,\n",
      "        [0.1185, 0.0283, 0.0108,  ..., 0.0135, 0.0538, 0.0141],\n",
      "        [0.0071, 0.0465, 0.0601,  ..., 0.1148, 0.0740, 0.0932],\n",
      "        [0.0116, 0.0305, 0.0147,  ..., 0.0123, 0.0027, 0.0175]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bigram_indices = xs_t[:, 0] * 27 + xs_t[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "xenc = F.one_hot(bigram_indices, 27*27).float() # one-hot encoding\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word: .cnbiltexfhfglyn.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define a function for sampling from the trained model\n",
    "def sample_trigram_model(W, stoi, itos, max_length=20, seed_chars=['.'], g=torch.Generator()):\n",
    "    \"\"\"\n",
    "    Generate a word using the trained trigram model.\n",
    "\n",
    "    Parameters:\n",
    "    - W: Trained weight matrix (729, 27)\n",
    "    - stoi: Dictionary mapping characters to indices\n",
    "    - itos: Dictionary mapping indices back to characters\n",
    "    - max_length: Maximum length of the generated sequence\n",
    "    - seed_chars: Starting character(s) for generation\n",
    "\n",
    "    Returns:\n",
    "    - Generated sequence as a string\n",
    "    \"\"\"\n",
    "    out = seed_chars[:]  # Initialize output with the seed characters\n",
    "\n",
    "    # Convert initial seed characters to indices\n",
    "    if len(seed_chars) == 1:\n",
    "        seed_chars.append(random.choice(list(stoi.keys())))  # Append a random character\n",
    "\n",
    "    ch1, ch2 = seed_chars[-2], seed_chars[-1]  # Start with the last two characters\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Convert (ch1, ch2) into a trigram index\n",
    "        trigram_index = stoi[ch1] * 27 + stoi[ch2]  # Encoding (bigram → single index)\n",
    "        xenc = F.one_hot(torch.tensor(trigram_index), num_classes=27 * 27).float()\n",
    "\n",
    "        # Forward pass: Compute probabilities for the next character\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum()  # Normalize probabilities\n",
    "\n",
    "        # Sample the next character\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        # Stop if the end token (\".\") is generated\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "        # Shift context for next trigram prediction\n",
    "        ch1, ch2 = ch2, itos[ix]\n",
    "\n",
    "    return ''.join(out)\n",
    "\n",
    "# Example usage\n",
    "generated_word = sample_trigram_model(W, stoi, itos)\n",
    "print(f\"Generated word: {generated_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated word: tah.\n",
      "Generated word: talithaedhtedarlnhrqbk\n",
      "Generated word: tala.\n",
      "Generated word: taglyan.\n",
      "Generated word: tayzkenpnsomspigogd.\n",
      "Generated word: tatavaniufoczlzduokowh\n",
      "Generated word: tarwdszskgroson.\n",
      "Generated word: taqmgmerlfddpqxplynuim\n",
      "Generated word: talie.\n",
      "Generated word: takwvhynnxbbzdotj.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    generated_word = sample_trigram_model(W, stoi, itos, seed_chars=['t', 'a'], g=g)\n",
    "    print(f\"Generated word: {generated_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_data(xs, ys, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1, seed=2147483647):\n",
    "    num_samples = xs.shape[0]\n",
    "    num_train = int(num_samples * train_ratio)\n",
    "    num_dev = int(num_samples * dev_ratio)\n",
    "    num_test = num_samples - num_train - num_dev\n",
    "\n",
    "    # shuffle before splitting\n",
    "    indices = torch.randperm(num_samples, generator=g)\n",
    "\n",
    "    xs_shuffled = xs[indices]\n",
    "    ys_shuffled = ys[indices]\n",
    "\n",
    "    # split the dataset\n",
    "    xs_train, xs_dev, xs_test = torch.split(xs_shuffled, [num_train, num_dev, num_test])\n",
    "    ys_train, ys_dev, ys_test = torch.split(ys_shuffled, [num_train, num_dev, num_test])\n",
    "    \n",
    "    return xs_train, xs_dev, xs_test, ys_train, ys_dev, ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train_b, xs_dev_b, xs_test_b, ys_train_b, ys_dev_b, ys_test_b = split_data(xs_b, ys_b)\n",
    "xs_train_t, xs_dev_t, xs_test_t, ys_train_t, ys_dev_t, ys_test_t = split_data(xs_t, ys_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W_t = torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "W_b = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5480940341949463\n",
      "2.5636048316955566\n",
      "2.5408473014831543\n",
      "2.5470118522644043\n",
      "2.539839267730713\n",
      "2.5565619468688965\n",
      "2.5335073471069336\n",
      "2.5397894382476807\n",
      "2.5338873863220215\n",
      "2.551547050476074\n"
     ]
    }
   ],
   "source": [
    "# gradient descent bigram\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs_train_b, 27).float() # one-hot encoding\n",
    "    logits = (xenc @ W_b) # log-counts\n",
    "    counts = logits.exp() # equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "    loss = -probs[torch.arange(len(ys_train_b)), ys_train_b].log().mean() + 0.01*(W_b**2).mean() # negative log-likelihood\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W_b.grad = None # set the gradients to zero\n",
    "    loss.backward() # backpropagate\n",
    "    \n",
    "    # update\n",
    "    W_b.data += -100 * W_b.grad # update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9147050380706787\n",
      "2.878364086151123\n",
      "2.845487117767334\n",
      "2.8156321048736572\n",
      "2.788423538208008\n",
      "2.7635366916656494\n",
      "2.7406864166259766\n",
      "2.7196226119995117\n",
      "2.7001261711120605\n",
      "2.6820075511932373\n"
     ]
    }
   ],
   "source": [
    "# gradient descent trigram\n",
    "for k in range(10):\n",
    "    # forward pass\n",
    "    bigram_indices = xs_train_t[:, 0] * 27 + xs_train_t[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "    xenc = F.one_hot(bigram_indices, 27*27).float() # one-hot encoding\n",
    "    \n",
    "    logits = (xenc @ W_t) # log-counts\n",
    "    counts = logits.exp() # equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "    loss = -probs[torch.arange(len(ys_train_t)), ys_train_t].log().mean() + 0.01*(W_t**2).mean() # negative log-likelihood\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W_t.grad = None # set the gradients to zero\n",
    "    loss.backward() # backpropagate\n",
    "    \n",
    "    # update\n",
    "    W_t.data += -100 * W_t.grad # update the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.545044422149658\n",
      "2.5196807384490967\n"
     ]
    }
   ],
   "source": [
    "# bigram dev & test loss\n",
    "xenc = F.one_hot(xs_dev_b, 27).float() # one-hot encoding\n",
    "logits = (xenc @ W_b) # log-counts\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_dev_b)), ys_dev_b].log().mean() + 0.01*(W_b**2).mean() # negative log-likelihood\n",
    "print(loss.item())\n",
    "\n",
    "xenc = F.one_hot(xs_test_b, 27).float() # one-hot encoding\n",
    "logits = (xenc @ W_b) # log-counts\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_test_b)), ys_test_b].log().mean() + 0.01*(W_b**2).mean() # negative log-likelihood\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6630184650421143\n",
      "2.682529926300049\n"
     ]
    }
   ],
   "source": [
    "# trigram dev & test loss\n",
    "bigram_indices = xs_dev_t[:, 0] * 27 + xs_dev_t[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "xenc = F.one_hot(bigram_indices, 27*27).float() # one-hot encoding\n",
    "\n",
    "logits = (xenc @ W_t) # log-counts\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_dev_t)), ys_dev_t].log().mean() + 0.01*(W_t**2).mean() # negative log-likelihood\n",
    "print(loss.item())\n",
    "\n",
    "bigram_indices = xs_test_t[:, 0] * 27 + xs_test_t[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "xenc = F.one_hot(bigram_indices, 27*27).float() # one-hot encoding\n",
    "\n",
    "logits = (xenc @ W_t) # log-counts\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_test_t)), ys_test_t].log().mean() + 0.01*(W_t**2).mean() # negative log-likelihood\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trigram_model(xs_train, ys_train, xs_dev, ys_dev, W_t, reg_strength, num_epochs=100, lr=10):\n",
    "    \"\"\"\n",
    "    Train the trigram model for a given regularization strength (smoothing).\n",
    "    \"\"\"\n",
    "    losses = {\"train\": [], \"dev\": []}  # Store losses for analysis\n",
    "    \n",
    "    for k in range(num_epochs):\n",
    "        # Forward pass\n",
    "        bigram_indices = xs_train[:, 0] * 27 + xs_train[:, 1]  # Convert bigram (ch1, ch2) into a single index\n",
    "        logits = W_t[bigram_indices]\n",
    "        counts = logits.exp()  # Equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdim=True)  # Probabilities for the next character\n",
    "        loss_train = -probs[torch.arange(len(ys_train)), ys_train].log().mean() + reg_strength * (W_t**2).mean()  # Negative log-likelihood\n",
    "        losses[\"train\"].append(loss_train.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        W_t.grad = None  # Set the gradients to zero\n",
    "        loss_train.backward()  # Backpropagate\n",
    "        with torch.no_grad():\n",
    "            W_t -= lr * W_t.grad\n",
    "            \n",
    "        # Compute dev loss\n",
    "        with torch.no_grad():\n",
    "            bigram_indices_dev = xs_dev[:, 0] * 27 + xs_dev[:, 1]\n",
    "            logits_dev = W_t[bigram_indices_dev]\n",
    "            counts_dev = logits_dev.exp()\n",
    "            probs_dev = counts_dev / counts_dev.sum(1, keepdim=True)\n",
    "            loss_dev = -probs_dev[torch.arange(len(ys_dev)), ys_dev].log().mean() + reg_strength * (W_t**2).mean()\n",
    "            losses[\"dev\"].append(loss_dev.item())\n",
    "            \n",
    "        # Print loss\n",
    "        print(f\"λ={reg_strength}, Epoch {k}: Train Loss = {loss_train.item():.4f}, Dev Loss = {loss_dev.item():.4f}\")\n",
    "        \n",
    "    return W_t, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with λ=0.001\n",
      "λ=0.001, Epoch 0: Train Loss = 3.7248, Dev Loss = 3.7042\n",
      "λ=0.001, Epoch 1: Train Loss = 3.7095, Dev Loss = 3.6890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ=0.001, Epoch 2: Train Loss = 3.6943, Dev Loss = 3.6740\n",
      "λ=0.001, Epoch 3: Train Loss = 3.6793, Dev Loss = 3.6592\n",
      "λ=0.001, Epoch 4: Train Loss = 3.6645, Dev Loss = 3.6445\n",
      "λ=0.001, Epoch 5: Train Loss = 3.6499, Dev Loss = 3.6301\n",
      "λ=0.001, Epoch 6: Train Loss = 3.6355, Dev Loss = 3.6158\n",
      "λ=0.001, Epoch 7: Train Loss = 3.6212, Dev Loss = 3.6018\n",
      "λ=0.001, Epoch 8: Train Loss = 3.6072, Dev Loss = 3.5879\n",
      "λ=0.001, Epoch 9: Train Loss = 3.5933, Dev Loss = 3.5742\n",
      "λ=0.001, Epoch 10: Train Loss = 3.5796, Dev Loss = 3.5607\n",
      "λ=0.001, Epoch 11: Train Loss = 3.5661, Dev Loss = 3.5473\n",
      "λ=0.001, Epoch 12: Train Loss = 3.5528, Dev Loss = 3.5342\n",
      "λ=0.001, Epoch 13: Train Loss = 3.5396, Dev Loss = 3.5212\n",
      "λ=0.001, Epoch 14: Train Loss = 3.5267, Dev Loss = 3.5084\n",
      "λ=0.001, Epoch 15: Train Loss = 3.5139, Dev Loss = 3.4958\n",
      "λ=0.001, Epoch 16: Train Loss = 3.5013, Dev Loss = 3.4834\n",
      "λ=0.001, Epoch 17: Train Loss = 3.4889, Dev Loss = 3.4712\n",
      "λ=0.001, Epoch 18: Train Loss = 3.4767, Dev Loss = 3.4591\n",
      "λ=0.001, Epoch 19: Train Loss = 3.4646, Dev Loss = 3.4473\n",
      "λ=0.001, Epoch 20: Train Loss = 3.4528, Dev Loss = 3.4356\n",
      "λ=0.001, Epoch 21: Train Loss = 3.4411, Dev Loss = 3.4241\n",
      "λ=0.001, Epoch 22: Train Loss = 3.4296, Dev Loss = 3.4128\n",
      "λ=0.001, Epoch 23: Train Loss = 3.4183, Dev Loss = 3.4017\n",
      "λ=0.001, Epoch 24: Train Loss = 3.4072, Dev Loss = 3.3907\n",
      "λ=0.001, Epoch 25: Train Loss = 3.3962, Dev Loss = 3.3800\n",
      "λ=0.001, Epoch 26: Train Loss = 3.3855, Dev Loss = 3.3694\n",
      "λ=0.001, Epoch 27: Train Loss = 3.3749, Dev Loss = 3.3590\n",
      "λ=0.001, Epoch 28: Train Loss = 3.3645, Dev Loss = 3.3487\n",
      "λ=0.001, Epoch 29: Train Loss = 3.3542, Dev Loss = 3.3386\n",
      "λ=0.001, Epoch 30: Train Loss = 3.3441, Dev Loss = 3.3287\n",
      "λ=0.001, Epoch 31: Train Loss = 3.3342, Dev Loss = 3.3190\n",
      "λ=0.001, Epoch 32: Train Loss = 3.3245, Dev Loss = 3.3094\n",
      "λ=0.001, Epoch 33: Train Loss = 3.3149, Dev Loss = 3.3000\n",
      "λ=0.001, Epoch 34: Train Loss = 3.3055, Dev Loss = 3.2908\n",
      "λ=0.001, Epoch 35: Train Loss = 3.2962, Dev Loss = 3.2817\n",
      "λ=0.001, Epoch 36: Train Loss = 3.2871, Dev Loss = 3.2727\n",
      "λ=0.001, Epoch 37: Train Loss = 3.2782, Dev Loss = 3.2640\n",
      "λ=0.001, Epoch 38: Train Loss = 3.2694, Dev Loss = 3.2553\n",
      "λ=0.001, Epoch 39: Train Loss = 3.2608, Dev Loss = 3.2468\n",
      "λ=0.001, Epoch 40: Train Loss = 3.2523, Dev Loss = 3.2385\n",
      "λ=0.001, Epoch 41: Train Loss = 3.2439, Dev Loss = 3.2303\n",
      "λ=0.001, Epoch 42: Train Loss = 3.2357, Dev Loss = 3.2222\n",
      "λ=0.001, Epoch 43: Train Loss = 3.2276, Dev Loss = 3.2142\n",
      "λ=0.001, Epoch 44: Train Loss = 3.2196, Dev Loss = 3.2064\n",
      "λ=0.001, Epoch 45: Train Loss = 3.2118, Dev Loss = 3.1987\n",
      "λ=0.001, Epoch 46: Train Loss = 3.2041, Dev Loss = 3.1912\n",
      "λ=0.001, Epoch 47: Train Loss = 3.1965, Dev Loss = 3.1837\n",
      "λ=0.001, Epoch 48: Train Loss = 3.1890, Dev Loss = 3.1764\n",
      "λ=0.001, Epoch 49: Train Loss = 3.1817, Dev Loss = 3.1692\n",
      "λ=0.001, Epoch 50: Train Loss = 3.1745, Dev Loss = 3.1621\n",
      "λ=0.001, Epoch 51: Train Loss = 3.1674, Dev Loss = 3.1551\n",
      "λ=0.001, Epoch 52: Train Loss = 3.1604, Dev Loss = 3.1482\n",
      "λ=0.001, Epoch 53: Train Loss = 3.1534, Dev Loss = 3.1414\n",
      "λ=0.001, Epoch 54: Train Loss = 3.1466, Dev Loss = 3.1347\n",
      "λ=0.001, Epoch 55: Train Loss = 3.1399, Dev Loss = 3.1282\n",
      "λ=0.001, Epoch 56: Train Loss = 3.1333, Dev Loss = 3.1217\n",
      "λ=0.001, Epoch 57: Train Loss = 3.1268, Dev Loss = 3.1153\n",
      "λ=0.001, Epoch 58: Train Loss = 3.1204, Dev Loss = 3.1090\n",
      "λ=0.001, Epoch 59: Train Loss = 3.1141, Dev Loss = 3.1027\n",
      "λ=0.001, Epoch 60: Train Loss = 3.1079, Dev Loss = 3.0966\n",
      "λ=0.001, Epoch 61: Train Loss = 3.1017, Dev Loss = 3.0906\n",
      "λ=0.001, Epoch 62: Train Loss = 3.0956, Dev Loss = 3.0846\n",
      "λ=0.001, Epoch 63: Train Loss = 3.0897, Dev Loss = 3.0787\n",
      "λ=0.001, Epoch 64: Train Loss = 3.0838, Dev Loss = 3.0729\n",
      "λ=0.001, Epoch 65: Train Loss = 3.0779, Dev Loss = 3.0672\n",
      "λ=0.001, Epoch 66: Train Loss = 3.0722, Dev Loss = 3.0615\n",
      "λ=0.001, Epoch 67: Train Loss = 3.0665, Dev Loss = 3.0559\n",
      "λ=0.001, Epoch 68: Train Loss = 3.0609, Dev Loss = 3.0504\n",
      "λ=0.001, Epoch 69: Train Loss = 3.0554, Dev Loss = 3.0450\n",
      "λ=0.001, Epoch 70: Train Loss = 3.0499, Dev Loss = 3.0396\n",
      "λ=0.001, Epoch 71: Train Loss = 3.0445, Dev Loss = 3.0343\n",
      "λ=0.001, Epoch 72: Train Loss = 3.0392, Dev Loss = 3.0291\n",
      "λ=0.001, Epoch 73: Train Loss = 3.0339, Dev Loss = 3.0239\n",
      "λ=0.001, Epoch 74: Train Loss = 3.0287, Dev Loss = 3.0188\n",
      "λ=0.001, Epoch 75: Train Loss = 3.0236, Dev Loss = 3.0137\n",
      "λ=0.001, Epoch 76: Train Loss = 3.0185, Dev Loss = 3.0087\n",
      "λ=0.001, Epoch 77: Train Loss = 3.0135, Dev Loss = 3.0038\n",
      "λ=0.001, Epoch 78: Train Loss = 3.0086, Dev Loss = 2.9989\n",
      "λ=0.001, Epoch 79: Train Loss = 3.0037, Dev Loss = 2.9941\n",
      "λ=0.001, Epoch 80: Train Loss = 2.9989, Dev Loss = 2.9894\n",
      "λ=0.001, Epoch 81: Train Loss = 2.9941, Dev Loss = 2.9847\n",
      "λ=0.001, Epoch 82: Train Loss = 2.9894, Dev Loss = 2.9800\n",
      "λ=0.001, Epoch 83: Train Loss = 2.9847, Dev Loss = 2.9754\n",
      "λ=0.001, Epoch 84: Train Loss = 2.9801, Dev Loss = 2.9709\n",
      "λ=0.001, Epoch 85: Train Loss = 2.9755, Dev Loss = 2.9664\n",
      "λ=0.001, Epoch 86: Train Loss = 2.9710, Dev Loss = 2.9619\n",
      "λ=0.001, Epoch 87: Train Loss = 2.9665, Dev Loss = 2.9575\n",
      "λ=0.001, Epoch 88: Train Loss = 2.9621, Dev Loss = 2.9532\n",
      "λ=0.001, Epoch 89: Train Loss = 2.9578, Dev Loss = 2.9489\n",
      "λ=0.001, Epoch 90: Train Loss = 2.9534, Dev Loss = 2.9446\n",
      "λ=0.001, Epoch 91: Train Loss = 2.9492, Dev Loss = 2.9404\n",
      "λ=0.001, Epoch 92: Train Loss = 2.9449, Dev Loss = 2.9363\n",
      "λ=0.001, Epoch 93: Train Loss = 2.9408, Dev Loss = 2.9322\n",
      "λ=0.001, Epoch 94: Train Loss = 2.9366, Dev Loss = 2.9281\n",
      "λ=0.001, Epoch 95: Train Loss = 2.9326, Dev Loss = 2.9241\n",
      "λ=0.001, Epoch 96: Train Loss = 2.9285, Dev Loss = 2.9201\n",
      "λ=0.001, Epoch 97: Train Loss = 2.9245, Dev Loss = 2.9162\n",
      "λ=0.001, Epoch 98: Train Loss = 2.9205, Dev Loss = 2.9123\n",
      "λ=0.001, Epoch 99: Train Loss = 2.9166, Dev Loss = 2.9084\n",
      "Training with λ=0.01\n",
      "λ=0.01, Epoch 0: Train Loss = 3.7339, Dev Loss = 3.7132\n",
      "λ=0.01, Epoch 1: Train Loss = 3.7185, Dev Loss = 3.6980\n",
      "λ=0.01, Epoch 2: Train Loss = 3.7033, Dev Loss = 3.6830\n",
      "λ=0.01, Epoch 3: Train Loss = 3.6883, Dev Loss = 3.6682\n",
      "λ=0.01, Epoch 4: Train Loss = 3.6735, Dev Loss = 3.6535\n",
      "λ=0.01, Epoch 5: Train Loss = 3.6589, Dev Loss = 3.6391\n",
      "λ=0.01, Epoch 6: Train Loss = 3.6444, Dev Loss = 3.6248\n",
      "λ=0.01, Epoch 7: Train Loss = 3.6302, Dev Loss = 3.6107\n",
      "λ=0.01, Epoch 8: Train Loss = 3.6161, Dev Loss = 3.5968\n",
      "λ=0.01, Epoch 9: Train Loss = 3.6022, Dev Loss = 3.5831\n",
      "λ=0.01, Epoch 10: Train Loss = 3.5885, Dev Loss = 3.5695\n",
      "λ=0.01, Epoch 11: Train Loss = 3.5750, Dev Loss = 3.5562\n",
      "λ=0.01, Epoch 12: Train Loss = 3.5616, Dev Loss = 3.5430\n",
      "λ=0.01, Epoch 13: Train Loss = 3.5485, Dev Loss = 3.5301\n",
      "λ=0.01, Epoch 14: Train Loss = 3.5355, Dev Loss = 3.5173\n",
      "λ=0.01, Epoch 15: Train Loss = 3.5227, Dev Loss = 3.5047\n",
      "λ=0.01, Epoch 16: Train Loss = 3.5101, Dev Loss = 3.4922\n",
      "λ=0.01, Epoch 17: Train Loss = 3.4977, Dev Loss = 3.4800\n",
      "λ=0.01, Epoch 18: Train Loss = 3.4855, Dev Loss = 3.4679\n",
      "λ=0.01, Epoch 19: Train Loss = 3.4734, Dev Loss = 3.4561\n",
      "λ=0.01, Epoch 20: Train Loss = 3.4616, Dev Loss = 3.4444\n",
      "λ=0.01, Epoch 21: Train Loss = 3.4499, Dev Loss = 3.4329\n",
      "λ=0.01, Epoch 22: Train Loss = 3.4384, Dev Loss = 3.4216\n",
      "λ=0.01, Epoch 23: Train Loss = 3.4271, Dev Loss = 3.4104\n",
      "λ=0.01, Epoch 24: Train Loss = 3.4159, Dev Loss = 3.3995\n",
      "λ=0.01, Epoch 25: Train Loss = 3.4050, Dev Loss = 3.3887\n",
      "λ=0.01, Epoch 26: Train Loss = 3.3942, Dev Loss = 3.3781\n",
      "λ=0.01, Epoch 27: Train Loss = 3.3836, Dev Loss = 3.3677\n",
      "λ=0.01, Epoch 28: Train Loss = 3.3732, Dev Loss = 3.3574\n",
      "λ=0.01, Epoch 29: Train Loss = 3.3629, Dev Loss = 3.3474\n",
      "λ=0.01, Epoch 30: Train Loss = 3.3529, Dev Loss = 3.3374\n",
      "λ=0.01, Epoch 31: Train Loss = 3.3429, Dev Loss = 3.3277\n",
      "λ=0.01, Epoch 32: Train Loss = 3.3332, Dev Loss = 3.3181\n",
      "λ=0.01, Epoch 33: Train Loss = 3.3236, Dev Loss = 3.3087\n",
      "λ=0.01, Epoch 34: Train Loss = 3.3142, Dev Loss = 3.2995\n",
      "λ=0.01, Epoch 35: Train Loss = 3.3049, Dev Loss = 3.2904\n",
      "λ=0.01, Epoch 36: Train Loss = 3.2958, Dev Loss = 3.2814\n",
      "λ=0.01, Epoch 37: Train Loss = 3.2869, Dev Loss = 3.2726\n",
      "λ=0.01, Epoch 38: Train Loss = 3.2781, Dev Loss = 3.2640\n",
      "λ=0.01, Epoch 39: Train Loss = 3.2694, Dev Loss = 3.2555\n",
      "λ=0.01, Epoch 40: Train Loss = 3.2609, Dev Loss = 3.2471\n",
      "λ=0.01, Epoch 41: Train Loss = 3.2525, Dev Loss = 3.2389\n",
      "λ=0.01, Epoch 42: Train Loss = 3.2443, Dev Loss = 3.2308\n",
      "λ=0.01, Epoch 43: Train Loss = 3.2362, Dev Loss = 3.2229\n",
      "λ=0.01, Epoch 44: Train Loss = 3.2282, Dev Loss = 3.2150\n",
      "λ=0.01, Epoch 45: Train Loss = 3.2204, Dev Loss = 3.2073\n",
      "λ=0.01, Epoch 46: Train Loss = 3.2127, Dev Loss = 3.1998\n",
      "λ=0.01, Epoch 47: Train Loss = 3.2051, Dev Loss = 3.1923\n",
      "λ=0.01, Epoch 48: Train Loss = 3.1976, Dev Loss = 3.1850\n",
      "λ=0.01, Epoch 49: Train Loss = 3.1903, Dev Loss = 3.1778\n",
      "λ=0.01, Epoch 50: Train Loss = 3.1831, Dev Loss = 3.1707\n",
      "λ=0.01, Epoch 51: Train Loss = 3.1759, Dev Loss = 3.1637\n",
      "λ=0.01, Epoch 52: Train Loss = 3.1689, Dev Loss = 3.1568\n",
      "λ=0.01, Epoch 53: Train Loss = 3.1620, Dev Loss = 3.1500\n",
      "λ=0.01, Epoch 54: Train Loss = 3.1552, Dev Loss = 3.1433\n",
      "λ=0.01, Epoch 55: Train Loss = 3.1485, Dev Loss = 3.1367\n",
      "λ=0.01, Epoch 56: Train Loss = 3.1419, Dev Loss = 3.1302\n",
      "λ=0.01, Epoch 57: Train Loss = 3.1354, Dev Loss = 3.1238\n",
      "λ=0.01, Epoch 58: Train Loss = 3.1290, Dev Loss = 3.1175\n",
      "λ=0.01, Epoch 59: Train Loss = 3.1226, Dev Loss = 3.1113\n",
      "λ=0.01, Epoch 60: Train Loss = 3.1164, Dev Loss = 3.1052\n",
      "λ=0.01, Epoch 61: Train Loss = 3.1102, Dev Loss = 3.0991\n",
      "λ=0.01, Epoch 62: Train Loss = 3.1042, Dev Loss = 3.0931\n",
      "λ=0.01, Epoch 63: Train Loss = 3.0982, Dev Loss = 3.0872\n",
      "λ=0.01, Epoch 64: Train Loss = 3.0923, Dev Loss = 3.0814\n",
      "λ=0.01, Epoch 65: Train Loss = 3.0865, Dev Loss = 3.0757\n",
      "λ=0.01, Epoch 66: Train Loss = 3.0807, Dev Loss = 3.0700\n",
      "λ=0.01, Epoch 67: Train Loss = 3.0750, Dev Loss = 3.0645\n",
      "λ=0.01, Epoch 68: Train Loss = 3.0694, Dev Loss = 3.0589\n",
      "λ=0.01, Epoch 69: Train Loss = 3.0639, Dev Loss = 3.0535\n",
      "λ=0.01, Epoch 70: Train Loss = 3.0584, Dev Loss = 3.0481\n",
      "λ=0.01, Epoch 71: Train Loss = 3.0530, Dev Loss = 3.0428\n",
      "λ=0.01, Epoch 72: Train Loss = 3.0477, Dev Loss = 3.0376\n",
      "λ=0.01, Epoch 73: Train Loss = 3.0424, Dev Loss = 3.0324\n",
      "λ=0.01, Epoch 74: Train Loss = 3.0372, Dev Loss = 3.0273\n",
      "λ=0.01, Epoch 75: Train Loss = 3.0321, Dev Loss = 3.0222\n",
      "λ=0.01, Epoch 76: Train Loss = 3.0270, Dev Loss = 3.0172\n",
      "λ=0.01, Epoch 77: Train Loss = 3.0220, Dev Loss = 3.0123\n",
      "λ=0.01, Epoch 78: Train Loss = 3.0171, Dev Loss = 3.0074\n",
      "λ=0.01, Epoch 79: Train Loss = 3.0122, Dev Loss = 3.0026\n",
      "λ=0.01, Epoch 80: Train Loss = 3.0073, Dev Loss = 2.9978\n",
      "λ=0.01, Epoch 81: Train Loss = 3.0026, Dev Loss = 2.9931\n",
      "λ=0.01, Epoch 82: Train Loss = 2.9978, Dev Loss = 2.9885\n",
      "λ=0.01, Epoch 83: Train Loss = 2.9932, Dev Loss = 2.9839\n",
      "λ=0.01, Epoch 84: Train Loss = 2.9885, Dev Loss = 2.9793\n",
      "λ=0.01, Epoch 85: Train Loss = 2.9840, Dev Loss = 2.9748\n",
      "λ=0.01, Epoch 86: Train Loss = 2.9795, Dev Loss = 2.9704\n",
      "λ=0.01, Epoch 87: Train Loss = 2.9750, Dev Loss = 2.9660\n",
      "λ=0.01, Epoch 88: Train Loss = 2.9706, Dev Loss = 2.9616\n",
      "λ=0.01, Epoch 89: Train Loss = 2.9662, Dev Loss = 2.9573\n",
      "λ=0.01, Epoch 90: Train Loss = 2.9619, Dev Loss = 2.9531\n",
      "λ=0.01, Epoch 91: Train Loss = 2.9576, Dev Loss = 2.9489\n",
      "λ=0.01, Epoch 92: Train Loss = 2.9534, Dev Loss = 2.9447\n",
      "λ=0.01, Epoch 93: Train Loss = 2.9492, Dev Loss = 2.9406\n",
      "λ=0.01, Epoch 94: Train Loss = 2.9451, Dev Loss = 2.9365\n",
      "λ=0.01, Epoch 95: Train Loss = 2.9410, Dev Loss = 2.9325\n",
      "λ=0.01, Epoch 96: Train Loss = 2.9369, Dev Loss = 2.9285\n",
      "λ=0.01, Epoch 97: Train Loss = 2.9329, Dev Loss = 2.9246\n",
      "λ=0.01, Epoch 98: Train Loss = 2.9290, Dev Loss = 2.9207\n",
      "λ=0.01, Epoch 99: Train Loss = 2.9251, Dev Loss = 2.9168\n",
      "Training with λ=0.1\n",
      "λ=0.1, Epoch 0: Train Loss = 3.8243, Dev Loss = 3.8034\n",
      "λ=0.1, Epoch 1: Train Loss = 3.8087, Dev Loss = 3.7881\n",
      "λ=0.1, Epoch 2: Train Loss = 3.7934, Dev Loss = 3.7729\n",
      "λ=0.1, Epoch 3: Train Loss = 3.7782, Dev Loss = 3.7579\n",
      "λ=0.1, Epoch 4: Train Loss = 3.7632, Dev Loss = 3.7431\n",
      "λ=0.1, Epoch 5: Train Loss = 3.7485, Dev Loss = 3.7285\n",
      "λ=0.1, Epoch 6: Train Loss = 3.7339, Dev Loss = 3.7141\n",
      "λ=0.1, Epoch 7: Train Loss = 3.7194, Dev Loss = 3.6998\n",
      "λ=0.1, Epoch 8: Train Loss = 3.7052, Dev Loss = 3.6858\n",
      "λ=0.1, Epoch 9: Train Loss = 3.6912, Dev Loss = 3.6719\n",
      "λ=0.1, Epoch 10: Train Loss = 3.6773, Dev Loss = 3.6582\n",
      "λ=0.1, Epoch 11: Train Loss = 3.6637, Dev Loss = 3.6448\n",
      "λ=0.1, Epoch 12: Train Loss = 3.6502, Dev Loss = 3.6315\n",
      "λ=0.1, Epoch 13: Train Loss = 3.6369, Dev Loss = 3.6184\n",
      "λ=0.1, Epoch 14: Train Loss = 3.6238, Dev Loss = 3.6054\n",
      "λ=0.1, Epoch 15: Train Loss = 3.6109, Dev Loss = 3.5927\n",
      "λ=0.1, Epoch 16: Train Loss = 3.5982, Dev Loss = 3.5802\n",
      "λ=0.1, Epoch 17: Train Loss = 3.5856, Dev Loss = 3.5678\n",
      "λ=0.1, Epoch 18: Train Loss = 3.5733, Dev Loss = 3.5556\n",
      "λ=0.1, Epoch 19: Train Loss = 3.5611, Dev Loss = 3.5437\n",
      "λ=0.1, Epoch 20: Train Loss = 3.5491, Dev Loss = 3.5319\n",
      "λ=0.1, Epoch 21: Train Loss = 3.5374, Dev Loss = 3.5203\n",
      "λ=0.1, Epoch 22: Train Loss = 3.5258, Dev Loss = 3.5088\n",
      "λ=0.1, Epoch 23: Train Loss = 3.5143, Dev Loss = 3.4976\n",
      "λ=0.1, Epoch 24: Train Loss = 3.5031, Dev Loss = 3.4865\n",
      "λ=0.1, Epoch 25: Train Loss = 3.4920, Dev Loss = 3.4757\n",
      "λ=0.1, Epoch 26: Train Loss = 3.4812, Dev Loss = 3.4650\n",
      "λ=0.1, Epoch 27: Train Loss = 3.4705, Dev Loss = 3.4545\n",
      "λ=0.1, Epoch 28: Train Loss = 3.4599, Dev Loss = 3.4441\n",
      "λ=0.1, Epoch 29: Train Loss = 3.4496, Dev Loss = 3.4339\n",
      "λ=0.1, Epoch 30: Train Loss = 3.4394, Dev Loss = 3.4239\n",
      "λ=0.1, Epoch 31: Train Loss = 3.4294, Dev Loss = 3.4141\n",
      "λ=0.1, Epoch 32: Train Loss = 3.4196, Dev Loss = 3.4044\n",
      "λ=0.1, Epoch 33: Train Loss = 3.4099, Dev Loss = 3.3949\n",
      "λ=0.1, Epoch 34: Train Loss = 3.4004, Dev Loss = 3.3856\n",
      "λ=0.1, Epoch 35: Train Loss = 3.3910, Dev Loss = 3.3764\n",
      "λ=0.1, Epoch 36: Train Loss = 3.3819, Dev Loss = 3.3674\n",
      "λ=0.1, Epoch 37: Train Loss = 3.3728, Dev Loss = 3.3585\n",
      "λ=0.1, Epoch 38: Train Loss = 3.3639, Dev Loss = 3.3498\n",
      "λ=0.1, Epoch 39: Train Loss = 3.3552, Dev Loss = 3.3412\n",
      "λ=0.1, Epoch 40: Train Loss = 3.3466, Dev Loss = 3.3328\n",
      "λ=0.1, Epoch 41: Train Loss = 3.3382, Dev Loss = 3.3245\n",
      "λ=0.1, Epoch 42: Train Loss = 3.3299, Dev Loss = 3.3163\n",
      "λ=0.1, Epoch 43: Train Loss = 3.3217, Dev Loss = 3.3083\n",
      "λ=0.1, Epoch 44: Train Loss = 3.3137, Dev Loss = 3.3004\n",
      "λ=0.1, Epoch 45: Train Loss = 3.3058, Dev Loss = 3.2927\n",
      "λ=0.1, Epoch 46: Train Loss = 3.2980, Dev Loss = 3.2850\n",
      "λ=0.1, Epoch 47: Train Loss = 3.2903, Dev Loss = 3.2775\n",
      "λ=0.1, Epoch 48: Train Loss = 3.2828, Dev Loss = 3.2701\n",
      "λ=0.1, Epoch 49: Train Loss = 3.2754, Dev Loss = 3.2628\n",
      "λ=0.1, Epoch 50: Train Loss = 3.2681, Dev Loss = 3.2556\n",
      "λ=0.1, Epoch 51: Train Loss = 3.2609, Dev Loss = 3.2486\n",
      "λ=0.1, Epoch 52: Train Loss = 3.2538, Dev Loss = 3.2416\n",
      "λ=0.1, Epoch 53: Train Loss = 3.2468, Dev Loss = 3.2348\n",
      "λ=0.1, Epoch 54: Train Loss = 3.2400, Dev Loss = 3.2280\n",
      "λ=0.1, Epoch 55: Train Loss = 3.2332, Dev Loss = 3.2214\n",
      "λ=0.1, Epoch 56: Train Loss = 3.2265, Dev Loss = 3.2148\n",
      "λ=0.1, Epoch 57: Train Loss = 3.2200, Dev Loss = 3.2084\n",
      "λ=0.1, Epoch 58: Train Loss = 3.2135, Dev Loss = 3.2020\n",
      "λ=0.1, Epoch 59: Train Loss = 3.2071, Dev Loss = 3.1957\n",
      "λ=0.1, Epoch 60: Train Loss = 3.2008, Dev Loss = 3.1895\n",
      "λ=0.1, Epoch 61: Train Loss = 3.1946, Dev Loss = 3.1834\n",
      "λ=0.1, Epoch 62: Train Loss = 3.1885, Dev Loss = 3.1774\n",
      "λ=0.1, Epoch 63: Train Loss = 3.1824, Dev Loss = 3.1714\n",
      "λ=0.1, Epoch 64: Train Loss = 3.1764, Dev Loss = 3.1656\n",
      "λ=0.1, Epoch 65: Train Loss = 3.1706, Dev Loss = 3.1598\n",
      "λ=0.1, Epoch 66: Train Loss = 3.1648, Dev Loss = 3.1541\n",
      "λ=0.1, Epoch 67: Train Loss = 3.1590, Dev Loss = 3.1484\n",
      "λ=0.1, Epoch 68: Train Loss = 3.1534, Dev Loss = 3.1429\n",
      "λ=0.1, Epoch 69: Train Loss = 3.1478, Dev Loss = 3.1374\n",
      "λ=0.1, Epoch 70: Train Loss = 3.1423, Dev Loss = 3.1320\n",
      "λ=0.1, Epoch 71: Train Loss = 3.1368, Dev Loss = 3.1266\n",
      "λ=0.1, Epoch 72: Train Loss = 3.1315, Dev Loss = 3.1213\n",
      "λ=0.1, Epoch 73: Train Loss = 3.1261, Dev Loss = 3.1161\n",
      "λ=0.1, Epoch 74: Train Loss = 3.1209, Dev Loss = 3.1109\n",
      "λ=0.1, Epoch 75: Train Loss = 3.1157, Dev Loss = 3.1058\n",
      "λ=0.1, Epoch 76: Train Loss = 3.1106, Dev Loss = 3.1008\n",
      "λ=0.1, Epoch 77: Train Loss = 3.1055, Dev Loss = 3.0958\n",
      "λ=0.1, Epoch 78: Train Loss = 3.1005, Dev Loss = 3.0909\n",
      "λ=0.1, Epoch 79: Train Loss = 3.0956, Dev Loss = 3.0860\n",
      "λ=0.1, Epoch 80: Train Loss = 3.0907, Dev Loss = 3.0812\n",
      "λ=0.1, Epoch 81: Train Loss = 3.0859, Dev Loss = 3.0765\n",
      "λ=0.1, Epoch 82: Train Loss = 3.0811, Dev Loss = 3.0718\n",
      "λ=0.1, Epoch 83: Train Loss = 3.0764, Dev Loss = 3.0671\n",
      "λ=0.1, Epoch 84: Train Loss = 3.0717, Dev Loss = 3.0625\n",
      "λ=0.1, Epoch 85: Train Loss = 3.0671, Dev Loss = 3.0580\n",
      "λ=0.1, Epoch 86: Train Loss = 3.0626, Dev Loss = 3.0535\n",
      "λ=0.1, Epoch 87: Train Loss = 3.0581, Dev Loss = 3.0491\n",
      "λ=0.1, Epoch 88: Train Loss = 3.0536, Dev Loss = 3.0447\n",
      "λ=0.1, Epoch 89: Train Loss = 3.0492, Dev Loss = 3.0404\n",
      "λ=0.1, Epoch 90: Train Loss = 3.0449, Dev Loss = 3.0361\n",
      "λ=0.1, Epoch 91: Train Loss = 3.0405, Dev Loss = 3.0318\n",
      "λ=0.1, Epoch 92: Train Loss = 3.0363, Dev Loss = 3.0276\n",
      "λ=0.1, Epoch 93: Train Loss = 3.0321, Dev Loss = 3.0235\n",
      "λ=0.1, Epoch 94: Train Loss = 3.0279, Dev Loss = 3.0194\n",
      "λ=0.1, Epoch 95: Train Loss = 3.0238, Dev Loss = 3.0153\n",
      "λ=0.1, Epoch 96: Train Loss = 3.0197, Dev Loss = 3.0113\n",
      "λ=0.1, Epoch 97: Train Loss = 3.0156, Dev Loss = 3.0073\n",
      "λ=0.1, Epoch 98: Train Loss = 3.0116, Dev Loss = 3.0034\n",
      "λ=0.1, Epoch 99: Train Loss = 3.0077, Dev Loss = 2.9995\n",
      "Training with λ=1\n",
      "λ=1, Epoch 0: Train Loss = 4.7284, Dev Loss = 4.7040\n",
      "λ=1, Epoch 1: Train Loss = 4.7093, Dev Loss = 4.6851\n",
      "λ=1, Epoch 2: Train Loss = 4.6904, Dev Loss = 4.6664\n",
      "λ=1, Epoch 3: Train Loss = 4.6718, Dev Loss = 4.6480\n",
      "λ=1, Epoch 4: Train Loss = 4.6534, Dev Loss = 4.6299\n",
      "λ=1, Epoch 5: Train Loss = 4.6352, Dev Loss = 4.6119\n",
      "λ=1, Epoch 6: Train Loss = 4.6173, Dev Loss = 4.5942\n",
      "λ=1, Epoch 7: Train Loss = 4.5996, Dev Loss = 4.5767\n",
      "λ=1, Epoch 8: Train Loss = 4.5821, Dev Loss = 4.5595\n",
      "λ=1, Epoch 9: Train Loss = 4.5648, Dev Loss = 4.5425\n",
      "λ=1, Epoch 10: Train Loss = 4.5478, Dev Loss = 4.5257\n",
      "λ=1, Epoch 11: Train Loss = 4.5310, Dev Loss = 4.5091\n",
      "λ=1, Epoch 12: Train Loss = 4.5145, Dev Loss = 4.4928\n",
      "λ=1, Epoch 13: Train Loss = 4.4981, Dev Loss = 4.4767\n",
      "λ=1, Epoch 14: Train Loss = 4.4820, Dev Loss = 4.4608\n",
      "λ=1, Epoch 15: Train Loss = 4.4662, Dev Loss = 4.4451\n",
      "λ=1, Epoch 16: Train Loss = 4.4505, Dev Loss = 4.4297\n",
      "λ=1, Epoch 17: Train Loss = 4.4351, Dev Loss = 4.4145\n",
      "λ=1, Epoch 18: Train Loss = 4.4198, Dev Loss = 4.3995\n",
      "λ=1, Epoch 19: Train Loss = 4.4048, Dev Loss = 4.3847\n",
      "λ=1, Epoch 20: Train Loss = 4.3901, Dev Loss = 4.3701\n",
      "λ=1, Epoch 21: Train Loss = 4.3755, Dev Loss = 4.3558\n",
      "λ=1, Epoch 22: Train Loss = 4.3611, Dev Loss = 4.3416\n",
      "λ=1, Epoch 23: Train Loss = 4.3470, Dev Loss = 4.3277\n",
      "λ=1, Epoch 24: Train Loss = 4.3331, Dev Loss = 4.3140\n",
      "λ=1, Epoch 25: Train Loss = 4.3194, Dev Loss = 4.3005\n",
      "λ=1, Epoch 26: Train Loss = 4.3059, Dev Loss = 4.2872\n",
      "λ=1, Epoch 27: Train Loss = 4.2925, Dev Loss = 4.2741\n",
      "λ=1, Epoch 28: Train Loss = 4.2794, Dev Loss = 4.2612\n",
      "λ=1, Epoch 29: Train Loss = 4.2665, Dev Loss = 4.2485\n",
      "λ=1, Epoch 30: Train Loss = 4.2538, Dev Loss = 4.2360\n",
      "λ=1, Epoch 31: Train Loss = 4.2413, Dev Loss = 4.2237\n",
      "λ=1, Epoch 32: Train Loss = 4.2290, Dev Loss = 4.2115\n",
      "λ=1, Epoch 33: Train Loss = 4.2168, Dev Loss = 4.1996\n",
      "λ=1, Epoch 34: Train Loss = 4.2049, Dev Loss = 4.1878\n",
      "λ=1, Epoch 35: Train Loss = 4.1931, Dev Loss = 4.1762\n",
      "λ=1, Epoch 36: Train Loss = 4.1815, Dev Loss = 4.1648\n",
      "λ=1, Epoch 37: Train Loss = 4.1701, Dev Loss = 4.1536\n",
      "λ=1, Epoch 38: Train Loss = 4.1588, Dev Loss = 4.1425\n",
      "λ=1, Epoch 39: Train Loss = 4.1477, Dev Loss = 4.1316\n",
      "λ=1, Epoch 40: Train Loss = 4.1368, Dev Loss = 4.1209\n",
      "λ=1, Epoch 41: Train Loss = 4.1261, Dev Loss = 4.1103\n",
      "λ=1, Epoch 42: Train Loss = 4.1155, Dev Loss = 4.0998\n",
      "λ=1, Epoch 43: Train Loss = 4.1050, Dev Loss = 4.0896\n",
      "λ=1, Epoch 44: Train Loss = 4.0947, Dev Loss = 4.0794\n",
      "λ=1, Epoch 45: Train Loss = 4.0846, Dev Loss = 4.0694\n",
      "λ=1, Epoch 46: Train Loss = 4.0746, Dev Loss = 4.0596\n",
      "λ=1, Epoch 47: Train Loss = 4.0647, Dev Loss = 4.0499\n",
      "λ=1, Epoch 48: Train Loss = 4.0549, Dev Loss = 4.0403\n",
      "λ=1, Epoch 49: Train Loss = 4.0453, Dev Loss = 4.0308\n",
      "λ=1, Epoch 50: Train Loss = 4.0359, Dev Loss = 4.0215\n",
      "λ=1, Epoch 51: Train Loss = 4.0265, Dev Loss = 4.0123\n",
      "λ=1, Epoch 52: Train Loss = 4.0173, Dev Loss = 4.0032\n",
      "λ=1, Epoch 53: Train Loss = 4.0082, Dev Loss = 3.9943\n",
      "λ=1, Epoch 54: Train Loss = 3.9992, Dev Loss = 3.9854\n",
      "λ=1, Epoch 55: Train Loss = 3.9904, Dev Loss = 3.9767\n",
      "λ=1, Epoch 56: Train Loss = 3.9816, Dev Loss = 3.9681\n",
      "λ=1, Epoch 57: Train Loss = 3.9730, Dev Loss = 3.9596\n",
      "λ=1, Epoch 58: Train Loss = 3.9644, Dev Loss = 3.9512\n",
      "λ=1, Epoch 59: Train Loss = 3.9560, Dev Loss = 3.9429\n",
      "λ=1, Epoch 60: Train Loss = 3.9477, Dev Loss = 3.9346\n",
      "λ=1, Epoch 61: Train Loss = 3.9394, Dev Loss = 3.9265\n",
      "λ=1, Epoch 62: Train Loss = 3.9313, Dev Loss = 3.9185\n",
      "λ=1, Epoch 63: Train Loss = 3.9233, Dev Loss = 3.9106\n",
      "λ=1, Epoch 64: Train Loss = 3.9153, Dev Loss = 3.9028\n",
      "λ=1, Epoch 65: Train Loss = 3.9075, Dev Loss = 3.8950\n",
      "λ=1, Epoch 66: Train Loss = 3.8997, Dev Loss = 3.8874\n",
      "λ=1, Epoch 67: Train Loss = 3.8920, Dev Loss = 3.8798\n",
      "λ=1, Epoch 68: Train Loss = 3.8844, Dev Loss = 3.8723\n",
      "λ=1, Epoch 69: Train Loss = 3.8769, Dev Loss = 3.8649\n",
      "λ=1, Epoch 70: Train Loss = 3.8695, Dev Loss = 3.8576\n",
      "λ=1, Epoch 71: Train Loss = 3.8622, Dev Loss = 3.8504\n",
      "λ=1, Epoch 72: Train Loss = 3.8549, Dev Loss = 3.8432\n",
      "λ=1, Epoch 73: Train Loss = 3.8477, Dev Loss = 3.8361\n",
      "λ=1, Epoch 74: Train Loss = 3.8406, Dev Loss = 3.8291\n",
      "λ=1, Epoch 75: Train Loss = 3.8336, Dev Loss = 3.8221\n",
      "λ=1, Epoch 76: Train Loss = 3.8266, Dev Loss = 3.8153\n",
      "λ=1, Epoch 77: Train Loss = 3.8197, Dev Loss = 3.8085\n",
      "λ=1, Epoch 78: Train Loss = 3.8129, Dev Loss = 3.8017\n",
      "λ=1, Epoch 79: Train Loss = 3.8061, Dev Loss = 3.7951\n",
      "λ=1, Epoch 80: Train Loss = 3.7994, Dev Loss = 3.7885\n",
      "λ=1, Epoch 81: Train Loss = 3.7928, Dev Loss = 3.7819\n",
      "λ=1, Epoch 82: Train Loss = 3.7863, Dev Loss = 3.7755\n",
      "λ=1, Epoch 83: Train Loss = 3.7798, Dev Loss = 3.7691\n",
      "λ=1, Epoch 84: Train Loss = 3.7734, Dev Loss = 3.7627\n",
      "λ=1, Epoch 85: Train Loss = 3.7670, Dev Loss = 3.7565\n",
      "λ=1, Epoch 86: Train Loss = 3.7607, Dev Loss = 3.7502\n",
      "λ=1, Epoch 87: Train Loss = 3.7544, Dev Loss = 3.7441\n",
      "λ=1, Epoch 88: Train Loss = 3.7483, Dev Loss = 3.7380\n",
      "λ=1, Epoch 89: Train Loss = 3.7421, Dev Loss = 3.7319\n",
      "λ=1, Epoch 90: Train Loss = 3.7361, Dev Loss = 3.7259\n",
      "λ=1, Epoch 91: Train Loss = 3.7301, Dev Loss = 3.7200\n",
      "λ=1, Epoch 92: Train Loss = 3.7241, Dev Loss = 3.7141\n",
      "λ=1, Epoch 93: Train Loss = 3.7182, Dev Loss = 3.7083\n",
      "λ=1, Epoch 94: Train Loss = 3.7123, Dev Loss = 3.7025\n",
      "λ=1, Epoch 95: Train Loss = 3.7066, Dev Loss = 3.6968\n",
      "λ=1, Epoch 96: Train Loss = 3.7008, Dev Loss = 3.6911\n",
      "λ=1, Epoch 97: Train Loss = 3.6951, Dev Loss = 3.6855\n",
      "λ=1, Epoch 98: Train Loss = 3.6895, Dev Loss = 3.6799\n",
      "λ=1, Epoch 99: Train Loss = 3.6839, Dev Loss = 3.6744\n",
      "Training with λ=10\n",
      "λ=10, Epoch 0: Train Loss = 13.7697, Dev Loss = 13.5288\n",
      "λ=10, Epoch 1: Train Loss = 13.5341, Dev Loss = 13.2984\n",
      "λ=10, Epoch 2: Train Loss = 13.3037, Dev Loss = 13.0732\n",
      "λ=10, Epoch 3: Train Loss = 13.0784, Dev Loss = 12.8530\n",
      "λ=10, Epoch 4: Train Loss = 12.8582, Dev Loss = 12.6377\n",
      "λ=10, Epoch 5: Train Loss = 12.6428, Dev Loss = 12.4272\n",
      "λ=10, Epoch 6: Train Loss = 12.4322, Dev Loss = 12.2213\n",
      "λ=10, Epoch 7: Train Loss = 12.2263, Dev Loss = 12.0200\n",
      "λ=10, Epoch 8: Train Loss = 12.0250, Dev Loss = 11.8232\n",
      "λ=10, Epoch 9: Train Loss = 11.8282, Dev Loss = 11.6307\n",
      "λ=10, Epoch 10: Train Loss = 11.6357, Dev Loss = 11.4425\n",
      "λ=10, Epoch 11: Train Loss = 11.4474, Dev Loss = 11.2585\n",
      "λ=10, Epoch 12: Train Loss = 11.2633, Dev Loss = 11.0785\n",
      "λ=10, Epoch 13: Train Loss = 11.0833, Dev Loss = 10.9025\n",
      "λ=10, Epoch 14: Train Loss = 10.9073, Dev Loss = 10.7304\n",
      "λ=10, Epoch 15: Train Loss = 10.7352, Dev Loss = 10.5622\n",
      "λ=10, Epoch 16: Train Loss = 10.5668, Dev Loss = 10.3976\n",
      "λ=10, Epoch 17: Train Loss = 10.4022, Dev Loss = 10.2366\n",
      "λ=10, Epoch 18: Train Loss = 10.2412, Dev Loss = 10.0792\n",
      "λ=10, Epoch 19: Train Loss = 10.0838, Dev Loss = 9.9253\n",
      "λ=10, Epoch 20: Train Loss = 9.9298, Dev Loss = 9.7748\n",
      "λ=10, Epoch 21: Train Loss = 9.7793, Dev Loss = 9.6276\n",
      "λ=10, Epoch 22: Train Loss = 9.6320, Dev Loss = 9.4836\n",
      "λ=10, Epoch 23: Train Loss = 9.4880, Dev Loss = 9.3428\n",
      "λ=10, Epoch 24: Train Loss = 9.3471, Dev Loss = 9.2051\n",
      "λ=10, Epoch 25: Train Loss = 9.2094, Dev Loss = 9.0704\n",
      "λ=10, Epoch 26: Train Loss = 9.0747, Dev Loss = 8.9386\n",
      "λ=10, Epoch 27: Train Loss = 8.9429, Dev Loss = 8.8098\n",
      "λ=10, Epoch 28: Train Loss = 8.8140, Dev Loss = 8.6838\n",
      "λ=10, Epoch 29: Train Loss = 8.6880, Dev Loss = 8.5606\n",
      "λ=10, Epoch 30: Train Loss = 8.5647, Dev Loss = 8.4400\n",
      "λ=10, Epoch 31: Train Loss = 8.4441, Dev Loss = 8.3221\n",
      "λ=10, Epoch 32: Train Loss = 8.3262, Dev Loss = 8.2068\n",
      "λ=10, Epoch 33: Train Loss = 8.2108, Dev Loss = 8.0940\n",
      "λ=10, Epoch 34: Train Loss = 8.0980, Dev Loss = 7.9837\n",
      "λ=10, Epoch 35: Train Loss = 7.9876, Dev Loss = 7.8758\n",
      "λ=10, Epoch 36: Train Loss = 7.8796, Dev Loss = 7.7702\n",
      "λ=10, Epoch 37: Train Loss = 7.7740, Dev Loss = 7.6669\n",
      "λ=10, Epoch 38: Train Loss = 7.6707, Dev Loss = 7.5659\n",
      "λ=10, Epoch 39: Train Loss = 7.5697, Dev Loss = 7.4671\n",
      "λ=10, Epoch 40: Train Loss = 7.4708, Dev Loss = 7.3705\n",
      "λ=10, Epoch 41: Train Loss = 7.3741, Dev Loss = 7.2759\n",
      "λ=10, Epoch 42: Train Loss = 7.2795, Dev Loss = 7.1834\n",
      "λ=10, Epoch 43: Train Loss = 7.1870, Dev Loss = 7.0929\n",
      "λ=10, Epoch 44: Train Loss = 7.0964, Dev Loss = 7.0044\n",
      "λ=10, Epoch 45: Train Loss = 7.0079, Dev Loss = 6.9178\n",
      "λ=10, Epoch 46: Train Loss = 6.9212, Dev Loss = 6.8331\n",
      "λ=10, Epoch 47: Train Loss = 6.8364, Dev Loss = 6.7502\n",
      "λ=10, Epoch 48: Train Loss = 6.7535, Dev Loss = 6.6691\n",
      "λ=10, Epoch 49: Train Loss = 6.6724, Dev Loss = 6.5897\n",
      "λ=10, Epoch 50: Train Loss = 6.5930, Dev Loss = 6.5121\n",
      "λ=10, Epoch 51: Train Loss = 6.5153, Dev Loss = 6.4361\n",
      "λ=10, Epoch 52: Train Loss = 6.4393, Dev Loss = 6.3618\n",
      "λ=10, Epoch 53: Train Loss = 6.3650, Dev Loss = 6.2891\n",
      "λ=10, Epoch 54: Train Loss = 6.2922, Dev Loss = 6.2180\n",
      "λ=10, Epoch 55: Train Loss = 6.2210, Dev Loss = 6.1484\n",
      "λ=10, Epoch 56: Train Loss = 6.1514, Dev Loss = 6.0802\n",
      "λ=10, Epoch 57: Train Loss = 6.0832, Dev Loss = 6.0136\n",
      "λ=10, Epoch 58: Train Loss = 6.0165, Dev Loss = 5.9484\n",
      "λ=10, Epoch 59: Train Loss = 5.9513, Dev Loss = 5.8846\n",
      "λ=10, Epoch 60: Train Loss = 5.8874, Dev Loss = 5.8221\n",
      "λ=10, Epoch 61: Train Loss = 5.8249, Dev Loss = 5.7610\n",
      "λ=10, Epoch 62: Train Loss = 5.7638, Dev Loss = 5.7012\n",
      "λ=10, Epoch 63: Train Loss = 5.7040, Dev Loss = 5.6427\n",
      "λ=10, Epoch 64: Train Loss = 5.6454, Dev Loss = 5.5855\n",
      "λ=10, Epoch 65: Train Loss = 5.5881, Dev Loss = 5.5294\n",
      "λ=10, Epoch 66: Train Loss = 5.5321, Dev Loss = 5.4746\n",
      "λ=10, Epoch 67: Train Loss = 5.4772, Dev Loss = 5.4209\n",
      "λ=10, Epoch 68: Train Loss = 5.4235, Dev Loss = 5.3684\n",
      "λ=10, Epoch 69: Train Loss = 5.3710, Dev Loss = 5.3170\n",
      "λ=10, Epoch 70: Train Loss = 5.3195, Dev Loss = 5.2667\n",
      "λ=10, Epoch 71: Train Loss = 5.2692, Dev Loss = 5.2175\n",
      "λ=10, Epoch 72: Train Loss = 5.2200, Dev Loss = 5.1694\n",
      "λ=10, Epoch 73: Train Loss = 5.1718, Dev Loss = 5.1222\n",
      "λ=10, Epoch 74: Train Loss = 5.1246, Dev Loss = 5.0761\n",
      "λ=10, Epoch 75: Train Loss = 5.0784, Dev Loss = 5.0310\n",
      "λ=10, Epoch 76: Train Loss = 5.0333, Dev Loss = 4.9868\n",
      "λ=10, Epoch 77: Train Loss = 4.9890, Dev Loss = 4.9435\n",
      "λ=10, Epoch 78: Train Loss = 4.9458, Dev Loss = 4.9012\n",
      "λ=10, Epoch 79: Train Loss = 4.9034, Dev Loss = 4.8598\n",
      "λ=10, Epoch 80: Train Loss = 4.8620, Dev Loss = 4.8192\n",
      "λ=10, Epoch 81: Train Loss = 4.8214, Dev Loss = 4.7796\n",
      "λ=10, Epoch 82: Train Loss = 4.7817, Dev Loss = 4.7407\n",
      "λ=10, Epoch 83: Train Loss = 4.7428, Dev Loss = 4.7027\n",
      "λ=10, Epoch 84: Train Loss = 4.7048, Dev Loss = 4.6655\n",
      "λ=10, Epoch 85: Train Loss = 4.6676, Dev Loss = 4.6291\n",
      "λ=10, Epoch 86: Train Loss = 4.6311, Dev Loss = 4.5935\n",
      "λ=10, Epoch 87: Train Loss = 4.5955, Dev Loss = 4.5586\n",
      "λ=10, Epoch 88: Train Loss = 4.5606, Dev Loss = 4.5245\n",
      "λ=10, Epoch 89: Train Loss = 4.5264, Dev Loss = 4.4911\n",
      "λ=10, Epoch 90: Train Loss = 4.4930, Dev Loss = 4.4584\n",
      "λ=10, Epoch 91: Train Loss = 4.4602, Dev Loss = 4.4263\n",
      "λ=10, Epoch 92: Train Loss = 4.4282, Dev Loss = 4.3950\n",
      "λ=10, Epoch 93: Train Loss = 4.3968, Dev Loss = 4.3643\n",
      "λ=10, Epoch 94: Train Loss = 4.3661, Dev Loss = 4.3343\n",
      "λ=10, Epoch 95: Train Loss = 4.3361, Dev Loss = 4.3049\n",
      "λ=10, Epoch 96: Train Loss = 4.3067, Dev Loss = 4.2762\n",
      "λ=10, Epoch 97: Train Loss = 4.2779, Dev Loss = 4.2480\n",
      "λ=10, Epoch 98: Train Loss = 4.2497, Dev Loss = 4.2205\n",
      "λ=10, Epoch 99: Train Loss = 4.2222, Dev Loss = 4.1935\n"
     ]
    }
   ],
   "source": [
    "# Try different regularization strengths\n",
    "reg_strengths = [0.001, 0.01, 0.1, 1, 10]  # Different lambda values\n",
    "best_loss = float('inf')\n",
    "best_W_t = None\n",
    "best_lambda = None\n",
    "all_losses = {}\n",
    "\n",
    "for reg_strength in reg_strengths:\n",
    "    print(f\"Training with λ={reg_strength}\")\n",
    "    \n",
    "    # Initialize the model parameters\n",
    "    W_t = torch.randn((27*27, 27), generator=torch.Generator().manual_seed(2147483647), requires_grad=True)\n",
    "    \n",
    "    # Train model\n",
    "    W_t, losses = train_trigram_model(xs_train_t, ys_train_t, xs_dev_t, ys_dev_t, W_t, reg_strength)\n",
    "    all_losses[reg_strength] = losses\n",
    "    \n",
    "    # Check if the current model is the best\n",
    "    if losses[\"dev\"][-1] < best_loss:\n",
    "        best_loss = losses[\"dev\"][-1]\n",
    "        best_W_t = W_t\n",
    "        best_lambda = reg_strength\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best regularization strength: λ = 0.001 with Dev Loss = 2.9084\n"
     ]
    }
   ],
   "source": [
    "# Print best λ found\n",
    "print(f\"\\nBest regularization strength: λ = {best_lambda} with Dev Loss = {best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss with best λ=0.001: 2.8541\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test set\n",
    "with torch.no_grad():\n",
    "    bigram_indices_test = xs_test_t[:, 0] * 27 + xs_test_t[:, 1]\n",
    "    logits_test = W_t[bigram_indices_test]\n",
    "    counts_test = logits_test.exp()\n",
    "    probs_test = counts_test / counts_test.sum(1, keepdim=True)\n",
    "\n",
    "    loss_test = -probs_test[torch.arange(len(ys_test_t)), ys_test_t].log().mean() + best_lambda * (best_W_t**2).mean()\n",
    "\n",
    "print(f\"\\nFinal Test Loss with best λ={best_lambda}: {loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss with best λ=0.001: 2.9254\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test set\n",
    "with torch.no_grad():\n",
    "    bigram_indices_test = xs_test_t[:, 0] * 27 + xs_test_t[:, 1]\n",
    "    logits_test = best_W_t[bigram_indices_test]\n",
    "    counts_test = logits_test.exp()\n",
    "    probs_test = counts_test / counts_test.sum(1, keepdim=True)\n",
    "\n",
    "    loss_test = -probs_test[torch.arange(len(ys_test_t)), ys_test_t].log().mean() + best_lambda * (best_W_t**2).mean()\n",
    "\n",
    "print(f\"\\nFinal Test Loss with best λ={best_lambda}: {loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.545044422149658\n",
      "2.5196807384490967\n"
     ]
    }
   ],
   "source": [
    "# bigram dev & test loss\n",
    "logits = W_b[xs_dev_b]\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_dev_b)), ys_dev_b].log().mean() + 0.01*(W_b**2).mean() # negative log-likelihood\n",
    "print(loss.item())\n",
    "\n",
    "logits = W_b[xs_test_b]\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts / counts.sum(1, keepdim=True) # probabilities for the next character\n",
    "loss = -probs[torch.arange(len(ys_test_b)), ys_test_b].log().mean() + 0.01*(W_b**2).mean() # negative log-likelihood\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss with best λ=0.001: 2.9254\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test set\n",
    "with torch.no_grad():\n",
    "    bigram_indices_test = xs_test_t[:, 0] * 27 + xs_test_t[:, 1]\n",
    "    logits_test = best_W_t[bigram_indices_test]\n",
    "    counts_test = logits_test.exp()\n",
    "    probs_test = counts_test / counts_test.sum(1, keepdim=True)\n",
    "\n",
    "    loss_test = F.cross_entropy(logits_test, ys_test_t) + best_lambda * (best_W_t**2).mean()\n",
    "\n",
    "print(f\"\\nFinal Test Loss with best λ={best_lambda}: {loss_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
